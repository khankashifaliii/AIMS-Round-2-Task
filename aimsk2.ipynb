{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cbcd2b2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-14T10:57:23.794042Z",
     "iopub.status.busy": "2025-08-14T10:57:23.793673Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-08-14T10:57:23.789316",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Setting up custom environment for YOLOv8 training...\n",
      "üì¶ Installing packages to avoid conflicts...\n",
      "Installing ultralytics...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ultralytics installed successfully\n",
      "Installing roboflow...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ roboflow installed successfully\n",
      "Installing opencv-python-headless...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ opencv-python-headless installed successfully\n",
      "Installing torch...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ torch installed successfully\n",
      "Installing torchvision...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ torchvision installed successfully\n",
      "Installing torchaudio...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"üöÄ Setting up custom environment for YOLOv8 training...\")\n",
    "\n",
    "# Create a custom directory for packages\n",
    "custom_site_packages = \"/kaggle/working/custom_packages\"\n",
    "os.makedirs(custom_site_packages, exist_ok=True)\n",
    "\n",
    "# Install packages to custom directory\n",
    "print(\"üì¶ Installing packages to avoid conflicts...\")\n",
    "packages = [\n",
    "    \"ultralytics\",\n",
    "    \"roboflow\", \n",
    "    \"opencv-python-headless\",\n",
    "    \"torch\",\n",
    "    \"torchvision\", \n",
    "    \"torchaudio\",\n",
    "    \"pillow\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"wandb\",\n",
    "    \"tensorboard\"\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    print(f\"Installing {package}...\")\n",
    "    try:\n",
    "        result = subprocess.run([\n",
    "            sys.executable, \"-m\", \"pip\", \"install\", \n",
    "            \"--target\", custom_site_packages,\n",
    "            \"--upgrade\", package\n",
    "        ], capture_output=True, text=True, check=True)\n",
    "        print(f\"‚úÖ {package} installed successfully\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Failed to install {package}\")\n",
    "        print(\"Error:\", e.stderr)\n",
    "\n",
    "# Install additional dependencies\n",
    "print(\"\\nüìö Installing additional dependencies...\")\n",
    "deps = [\"scipy\", \"requests\", \"tqdm\", \"psutil\", \"py-cpuinfo\", \"pyyaml\"]\n",
    "for dep in deps:\n",
    "    try:\n",
    "        subprocess.run([\n",
    "            sys.executable, \"-m\", \"pip\", \"install\", \n",
    "            \"--target\", custom_site_packages,\n",
    "            \"--upgrade\", dep\n",
    "        ], check=True, capture_output=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f\"\\n‚úÖ Custom packages installed in: {custom_site_packages}\")\n",
    "\n",
    "# Function to setup environment\n",
    "def setup_custom_env():\n",
    "    \"\"\"Add custom packages to Python path\"\"\"\n",
    "    if custom_site_packages not in sys.path:\n",
    "        sys.path.insert(0, custom_site_packages)\n",
    "    print(\"üîß Custom environment activated!\")\n",
    "\n",
    "# Test installations\n",
    "print(\"\\nüß™ Testing installations...\")\n",
    "setup_custom_env()\n",
    "\n",
    "test_results = {}\n",
    "\n",
    "# Test imports\n",
    "test_packages = [\n",
    "    ('cv2', 'OpenCV'),\n",
    "    ('roboflow', 'Roboflow'),\n",
    "    ('torch', 'PyTorch'),\n",
    "    ('ultralytics', 'Ultralytics')\n",
    "]\n",
    "\n",
    "for module, name in test_packages:\n",
    "    try:\n",
    "        exec(f\"import {module}\")\n",
    "        if module == 'cv2':\n",
    "            exec(f\"version = {module}.__version__\")\n",
    "        elif module == 'torch':\n",
    "            exec(f\"version = {module}.__version__\")\n",
    "        else:\n",
    "            version = \"imported\"\n",
    "        test_results[name] = f\"‚úÖ {version}\"\n",
    "    except Exception as e:\n",
    "        test_results[name] = f\"‚ùå {str(e)[:50]}\"\n",
    "\n",
    "print(\"\\nüìã Installation Results:\")\n",
    "for name, result in test_results.items():\n",
    "    print(f\"  {name}: {result}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ ENVIRONMENT SETUP COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Next: Run Cell 2 to download your dataset from Roboflow\")#!/usr/bin/env python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5d859d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T08:10:01.093546Z",
     "iopub.status.busy": "2025-08-14T08:10:01.092716Z",
     "iopub.status.idle": "2025-08-14T08:10:38.979681Z",
     "shell.execute_reply": "2025-08-14T08:10:38.978704Z",
     "shell.execute_reply.started": "2025-08-14T08:10:01.093518Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup custom environment\n",
    "import sys\n",
    "custom_site_packages = \"/kaggle/working/custom_packages\"\n",
    "if custom_site_packages not in sys.path:\n",
    "    sys.path.insert(0, custom_site_packages)\n",
    "\n",
    "import os\n",
    "from roboflow import Roboflow\n",
    "\n",
    "\n",
    "!pip install roboflow\n",
    "\n",
    "\n",
    "print(\"üì• Downloading dataset from Roboflow...\")\n",
    "\n",
    "# REPLACE THESE WITH YOUR ROBOFLOW DETAILS\n",
    "ROBOFLOW_API_KEY = \"mFwrI2EyU1GiHKaOGlxI\"  # Replace with your API key\n",
    "WORKSPACE_NAME = \"aims-ipbxa\"        # Replace with your workspace name\n",
    "PROJECT_NAME = \"activity-cswy1-weawp\"           # Replace with your project name  \n",
    "VERSION_NUMBER = 2                      # Replace with your dataset version\n",
    "\n",
    "# Initialize Roboflow\n",
    "try:\n",
    "    rf = Roboflow(api_key=ROBOFLOW_API_KEY)\n",
    "    print(f\"‚úÖ Connected to Roboflow with API key: {ROBOFLOW_API_KEY[:8]}...\")\n",
    "    \n",
    "    # Get project\n",
    "    project = rf.workspace(WORKSPACE_NAME).project(PROJECT_NAME)\n",
    "    print(f\"‚úÖ Found project: {PROJECT_NAME}\")\n",
    "    \n",
    "    # Download dataset\n",
    "    dataset_path = \"/kaggle/working/dataset\"\n",
    "    dataset = project.version(VERSION_NUMBER).download(\"yolov8\", location=dataset_path)\n",
    "    print(f\"‚úÖ Dataset downloaded to: {dataset_path}\")\n",
    "    \n",
    "    # Verify dataset structure\n",
    "    print(\"\\nüìÅ Dataset structure:\")\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        level = root.replace(dataset_path, '').count(os.sep)\n",
    "        indent = ' ' * 2 * level\n",
    "        print(f'{indent}{os.path.basename(root)}/')\n",
    "        subindent = ' ' * 2 * (level + 1)\n",
    "        for file in files[:5]:  # Show first 5 files in each directory\n",
    "            print(f'{subindent}{file}')\n",
    "        if len(files) > 5:\n",
    "            print(f'{subindent}... and {len(files) - 5} more files')\n",
    "    \n",
    "    # Find data.yaml file\n",
    "    data_yaml_path = None\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        if 'data.yaml' in files:\n",
    "            data_yaml_path = os.path.join(root, 'data.yaml')\n",
    "            break\n",
    "    \n",
    "    if data_yaml_path:\n",
    "        print(f\"\\n‚úÖ Found data.yaml at: {data_yaml_path}\")\n",
    "        \n",
    "        # Read and display data.yaml content\n",
    "        with open(data_yaml_path, 'r') as f:\n",
    "            yaml_content = f.read()\n",
    "        print(\"\\nüìÑ data.yaml content:\")\n",
    "        print(yaml_content)\n",
    "        \n",
    "        # Store the path for later use\n",
    "        os.environ[\"DATA_YAML_PATH\"] = data_yaml_path\n",
    "        print(f\"\\nüíæ Data YAML path saved to environment: {data_yaml_path}\")\n",
    "    else:\n",
    "        print(\"‚ùå data.yaml not found in dataset!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error downloading dataset: {str(e)}\")\n",
    "    print(\"\\nüí° Make sure to:\")\n",
    "    print(\"1. Replace ROBOFLOW_API_KEY with your actual API key\")\n",
    "    print(\"2. Replace WORKSPACE_NAME with your workspace name\")  \n",
    "    print(\"3. Replace PROJECT_NAME with your project name\")\n",
    "    print(\"4. Set correct VERSION_NUMBER\")\n",
    "    print(\"\\nüîó Get your API key from: https://roboflow.com/settings/api\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Next: Run Cell 3 to create the training script\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780b0561",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T08:15:34.669767Z",
     "iopub.status.busy": "2025-08-14T08:15:34.668882Z",
     "iopub.status.idle": "2025-08-14T08:15:34.680720Z",
     "shell.execute_reply": "2025-08-14T08:15:34.679809Z",
     "shell.execute_reply.started": "2025-08-14T08:15:34.669733Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile train_yolov8_ddp.py\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add custom packages path\n",
    "custom_packages = '/kaggle/working/custom_packages'\n",
    "if custom_packages not in sys.path:\n",
    "    sys.path.insert(0, custom_packages)\n",
    "\n",
    "# Import after adding the path\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "    import torch\n",
    "    print(\"‚úÖ Ultralytics and PyTorch imported successfully\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Failed to import required packages: {e}\")\n",
    "    print(\"Current Python path:\")\n",
    "    for path in sys.path:\n",
    "        print(f\"  {path}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "def main():\n",
    "    print(\"üöÄ Starting YOLOv8m DDP training...\")\n",
    "    \n",
    "    # Environment setup for better logging\n",
    "    os.environ[\"PYTHONUNBUFFERED\"] = \"1\"\n",
    "    os.environ[\"ULTRALYTICS_VERBOSE\"] = \"1\"\n",
    "    \n",
    "    # Get data.yaml path from environment or use default\n",
    "    data_yaml = os.environ.get(\"DATA_YAML_PATH\", \"/kaggle/working/dataset/data.yaml\")\n",
    "    \n",
    "    if not os.path.exists(data_yaml):\n",
    "        print(f\"‚ùå Data YAML file not found at: {data_yaml}\")\n",
    "        print(\"Please check the path and run the dataset download cell first.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìä Using dataset: {data_yaml}\")\n",
    "    \n",
    "    # Load YOLOv8m model\n",
    "    print(\"üì• Loading YOLOv8m model...\")\n",
    "    model = YOLO(\"yolov8m.pt\")  # This will download the pretrained weights\n",
    "    \n",
    "    print(\"üîß Training configuration:\")\n",
    "    config = {\n",
    "        'data': data_yaml,\n",
    "        'epochs': 20,           # Adjust based on your needs\n",
    "        'imgsz': 800,           # Image size\n",
    "        'batch': 44,            # Total batch size (will be split across GPUs)\n",
    "        'device': [0, 1],       # Use both T4 GPUs\n",
    "        'workers': 8,           # Data loading workers\n",
    "        'project': '/kaggle/working/runs',  # Where to save results\n",
    "        'name': 'yolov8m_ddp', # Experiment name\n",
    "        'save': True,\n",
    "        'save_period': 10,      # Save checkpoint every 10 epochs\n",
    "        'plots': True,\n",
    "        'verbose': True,\n",
    "        'patience': 10,         # Early stopping patience\n",
    "        'lr0': 0.01,           # Initial learning rate\n",
    "        'weight_decay': 0.0005,\n",
    "        'warmup_epochs': 3,\n",
    "        'box': 7.5,            # Box loss weight\n",
    "        'cls': 0.5,            # Class loss weight  \n",
    "        'dfl': 1.5,            # Distribution focal loss weight\n",
    "        'pose': 12.0,          # Pose loss weight (if using pose model)\n",
    "        'kobj': 1.0,           # Keypoint object loss weight\n",
    "        'label_smoothing': 0.0,\n",
    "        'nbs': 64,             # Nominal batch size\n",
    "        'overlap_mask': True,\n",
    "        'mask_ratio': 4,\n",
    "        'dropout': 0.0,\n",
    "        'val': True,           # Validate during training\n",
    "        'split': 'val',        # Dataset split to use for validation\n",
    "        'resume': False,       # Resume from last checkpoint\n",
    "        'amp': True,           # Automatic mixed precision\n",
    "        'fraction': 1.0,       # Dataset fraction to use\n",
    "        'profile': False,      # Profile ONNX and TensorRT speeds\n",
    "        'freeze': None,        # Freeze layers: backbone=10, all=24\n",
    "        'multi_scale': False,  # Multi-scale training\n",
    "        'optimizer': 'auto',   # Optimizer (SGD, Adam, AdamW, NAdam, RAdam, RMSProp)\n",
    "        'cos_lr': False,       # Use cosine learning rate scheduler\n",
    "        'close_mosaic': 10,    # Disable mosaic augmentation for final epochs\n",
    "        'single_cls': False,   # Train multi-class data as single-class\n",
    "        'rect': False,         # Rectangular training\n",
    "        'deterministic': True, # Force deterministic augmentation\n",
    "        # 'sync_bn': True,       # Use SyncBatchNorm, only available in DDP mode\n",
    "        'exist_ok': False,     # Overwrite existing experiment\n",
    "        'seed': 0,             # Global training seed\n",
    "        # 'local_rank': -1,      # Automatic DDP Multi-GPU argument, do not modify\n",
    "        'cache': False,        # True/ram, disk or False. Use cache for data loading\n",
    "        'visualize': False,    # Visualize features\n",
    "        'augment': True,       # Apply image augmentation to prediction sources\n",
    "        'agnostic_nms': False, # Class-agnostic NMS\n",
    "        'retina_masks': False, # Use high-resolution segmentation masks\n",
    "    }\n",
    "    \n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(\"\\nüèãÔ∏è Starting training...\")\n",
    "    print(\"This will use both T4 GPUs with DDP automatically\")\n",
    "    \n",
    "    try:\n",
    "        # Start training\n",
    "        results = model.train(**config)\n",
    "        \n",
    "        print(\"\\nüéâ Training completed successfully!\")\n",
    "        print(f\"Results saved to: {config['project']}/{config['name']}\")\n",
    "        \n",
    "        # Display some results\n",
    "        if results:\n",
    "            print(\"\\nüìà Training Results:\")\n",
    "            if hasattr(results, 'results_dict'):\n",
    "                for key, value in results.results_dict.items():\n",
    "                    if isinstance(value, (int, float)):\n",
    "                        print(f\"  {key}: {value:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Training failed: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fdab21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T08:11:25.766362Z",
     "iopub.status.busy": "2025-08-14T08:11:25.766020Z",
     "iopub.status.idle": "2025-08-14T08:11:26.371976Z",
     "shell.execute_reply": "2025-08-14T08:11:26.371232Z",
     "shell.execute_reply.started": "2025-08-14T08:11:25.766331Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup custom environment\n",
    "import sys\n",
    "custom_site_packages = \"/kaggle/working/custom_packages\"\n",
    "if custom_site_packages not in sys.path:\n",
    "    sys.path.insert(0, custom_site_packages)\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "print(\"üîç System and GPU Information\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check CUDA and PyTorch\n",
    "print(\"üêç Python & PyTorch Info:\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "# GPU details\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\nüéÆ GPU Details:\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "        print(f\"  GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "        \n",
    "        # Set GPU and check memory\n",
    "        torch.cuda.set_device(i)\n",
    "        memory_allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "        memory_reserved = torch.cuda.memory_reserved(i) / 1024**3\n",
    "        print(f\"    Memory allocated: {memory_allocated:.2f} GB\")\n",
    "        print(f\"    Memory reserved: {memory_reserved:.2f} GB\")\n",
    "        print(f\"    Memory free: {gpu_memory - memory_reserved:.2f} GB\")\n",
    "\n",
    "# Check NVIDIA-SMI\n",
    "print(\"\\nüñ•Ô∏è NVIDIA System Info:\")\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        print(\"‚ùå nvidia-smi not available\")\n",
    "except:\n",
    "    print(\"‚ùå nvidia-smi command failed\")\n",
    "\n",
    "# Check dataset path\n",
    "data_yaml_path = os.environ.get(\"DATA_YAML_PATH\")\n",
    "if data_yaml_path and os.path.exists(data_yaml_path):\n",
    "    print(f\"\\n‚úÖ Dataset ready at: {data_yaml_path}\")\n",
    "    \n",
    "    # Count images in dataset\n",
    "    dataset_dir = os.path.dirname(data_yaml_path)\n",
    "    train_dir = os.path.join(dataset_dir, 'train', 'images')\n",
    "    val_dir = os.path.join(dataset_dir, 'valid', 'images') \n",
    "    test_dir = os.path.join(dataset_dir, 'test', 'images')\n",
    "    \n",
    "    train_count = len(os.listdir(train_dir)) if os.path.exists(train_dir) else 0\n",
    "    val_count = len(os.listdir(val_dir)) if os.path.exists(val_dir) else 0\n",
    "    test_count = len(os.listdir(test_dir)) if os.path.exists(test_dir) else 0\n",
    "    \n",
    "    print(f\"üìä Dataset Statistics:\")\n",
    "    print(f\"  Training images: {train_count}\")\n",
    "    print(f\"  Validation images: {val_count}\")\n",
    "    print(f\"  Test images: {test_count}\")\n",
    "    print(f\"  Total images: {train_count + val_count + test_count}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Dataset not found. Please run the dataset download cell first.\")\n",
    "\n",
    "# Test DDP availability\n",
    "print(\"\\nüîó Distributed Training Check:\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"‚úÖ Multiple GPUs detected: {torch.cuda.device_count()} GPUs\")\n",
    "    print(\"‚úÖ DDP (Distributed Data Parallel) will be used automatically\")\n",
    "    \n",
    "    # Test multi-GPU tensor\n",
    "    if torch.cuda.is_available():\n",
    "        device0 = torch.device('cuda:0')\n",
    "        device1 = torch.device('cuda:1')\n",
    "        \n",
    "        # Create tensors on both GPUs\n",
    "        tensor0 = torch.randn(1000, 1000, device=device0)\n",
    "        tensor1 = torch.randn(1000, 1000, device=device1)\n",
    "        \n",
    "        print(\"‚úÖ Successfully created tensors on both GPUs\")\n",
    "        \n",
    "        # Clean up\n",
    "        del tensor0, tensor1\n",
    "        torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Only 1 GPU detected. DDP will not be used.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ Ready to start training!\")\n",
    "print(\"Run Cell 5 to begin YOLOv8m training with DDP\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b628036",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T08:15:53.483210Z",
     "iopub.status.busy": "2025-08-14T08:15:53.482934Z",
     "iopub.status.idle": "2025-08-14T10:56:07.641830Z",
     "shell.execute_reply": "2025-08-14T10:56:07.640926Z",
     "shell.execute_reply.started": "2025-08-14T08:15:53.483192Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"üöÄ Starting YOLOv8m DDP Training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if dataset is ready\n",
    "data_yaml_path = os.environ.get(\"DATA_YAML_PATH\")\n",
    "if not data_yaml_path or not os.path.exists(data_yaml_path):\n",
    "    print(\"‚ùå Dataset not found! Please run the dataset download cell first.\")\n",
    "    print(\"Expected path:\", data_yaml_path)\n",
    "    exit(1)\n",
    "\n",
    "print(f\"‚úÖ Dataset found: {data_yaml_path}\")\n",
    "\n",
    "# Set up environment for the training process\n",
    "env = os.environ.copy()\n",
    "env[\"DATA_YAML_PATH\"] = data_yaml_path\n",
    "env[\"PYTHONPATH\"] = \"/kaggle/working/custom_packages\"\n",
    "env[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # Use both GPUs\n",
    "\n",
    "# Check if training script exists\n",
    "if not os.path.exists(\"/kaggle/working/train_yolov8_ddp.py\"):\n",
    "    print(\"‚ùå Training script not found! Please run Cell 3 first.\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"‚úÖ Training script found\")\n",
    "print(\"üîß Environment setup:\")\n",
    "print(f\"  Data YAML: {data_yaml_path}\")\n",
    "print(f\"  Python Path: {env.get('PYTHONPATH')}\")\n",
    "print(f\"  CUDA Devices: {env.get('CUDA_VISIBLE_DEVICES')}\")\n",
    "\n",
    "# Start training\n",
    "print(\"\\nüèãÔ∏è Launching training process...\")\n",
    "print(\"This may take several hours depending on your dataset size and epochs.\")\n",
    "print(\"Training will use both T4 GPUs automatically with DDP.\")\n",
    "\n",
    "try:\n",
    "    # Run the training script\n",
    "    process = subprocess.Popen([\n",
    "        sys.executable, \"/kaggle/working/train_yolov8_ddp.py\"\n",
    "    ], env=env, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, \n",
    "       universal_newlines=True, cwd=\"/kaggle/working\")\n",
    "    \n",
    "    # Stream output in real-time\n",
    "    for line in process.stdout:\n",
    "        print(line.rstrip())\n",
    "    \n",
    "    # Wait for completion\n",
    "    return_code = process.wait()\n",
    "    \n",
    "    if return_code == 0:\n",
    "        print(\"\\nüéâ Training completed successfully!\")\n",
    "        \n",
    "        # Check for results\n",
    "        results_dir = \"/kaggle/working/runs/detect/yolov8m_ddp\"\n",
    "        if os.path.exists(results_dir):\n",
    "            print(f\"‚úÖ Results saved to: {results_dir}\")\n",
    "            \n",
    "            # List key files\n",
    "            key_files = ['weights/best.pt', 'weights/last.pt', 'results.png', 'confusion_matrix.png']\n",
    "            print(\"\\nüìÅ Key output files:\")\n",
    "            for file in key_files:\n",
    "                full_path = os.path.join(results_dir, file)\n",
    "                if os.path.exists(full_path):\n",
    "                    print(f\"  ‚úÖ {file}\")\n",
    "                else:\n",
    "                    print(f\"  ‚ùå {file} (not found)\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Results directory not found\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Training failed with return code: {return_code}\")\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö†Ô∏è Training interrupted by user\")\n",
    "    if 'process' in locals():\n",
    "        process.terminate()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error during training: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training process completed!\")\n",
    "print(\"Check the output above for results and any errors.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7112eb8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-14T10:57:18.238526",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}